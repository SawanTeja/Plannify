const express = require('express');
const router = express.Router();
const authMiddleware = require('../middleware/authMiddleware');

// Import ALL Models
const Task = require('../models/Task');
const Habit = require('../models/Habit');
const Transaction = require('../models/Transaction');
const Journal = require('../models/Journal');
const Subject = require('../models/Subject');      // <-- NEW
const Timetable = require('../models/Timetable');  // <-- NEW
const BucketItem = require('../models/BucketItem');// <-- NEW
const Gamification = require('../models/Gamification');
const Budget = require('../models/Budget'); // <-- NEW
const SocialPost = require('../models/SocialPost');
const SocialGroup = require('../models/SocialGroup');
const SplitGroup = require('../models/SplitGroup');
const SplitExpense = require('../models/SplitExpense');

const cloudinary = require('cloudinary').v2;

// Configure Cloudinary
cloudinary.config({
  cloud_name: process.env.CLOUDINARY_CLOUD_NAME,
  api_key: process.env.CLOUDINARY_API_KEY,
  api_secret: process.env.CLOUDINARY_API_SECRET
});

// Helper to extract public_id
const extractPublicId = (url) => {
  if (!url || !url.includes('cloudinary.com')) return null;
  try {
    const match = url.match(/\/upload\/(?:v\d+\/)?(.+)\.\w+$/);
    if (match && match[1]) return match[1];
  } catch (e) {
    console.error('Error extracting public_id:', e);
  }
  return null;
};

// Helper to handle Cloudinary cleanup for synced deletions (Journal)
async function processJournalDeletions(userId, items) {
  if (!items || items.length === 0) return;
  
  // Find items marked as deleted in this sync payload
  const deletedItems = items.filter(i => i.isDeleted);
  if (deletedItems.length === 0) return;

  try {
     const ids = deletedItems.map(i => i._id);
     // Fetch existing docs to get their image URLs before we potentially overwrite/ignore them
     const docs = await Journal.find({ _id: { $in: ids }, userId }).lean();
     
     const publicIds = [];
     docs.forEach(doc => {
         if (doc.image && doc.image.includes('cloudinary.com')) {
             const pid = extractPublicId(doc.image);
             if (pid) publicIds.push(pid);
         }
     });

     if (publicIds.length > 0) {
         console.log(`☁️ Sync: Cleaning up ${publicIds.length} images for deleted journals`);
         // Try batch delete
         try {
             await cloudinary.api.delete_resources(publicIds);
         } catch (batchErr) {
             console.error("Batch delete failed, trying individual:", batchErr.message);
             await Promise.all(publicIds.map(id => cloudinary.uploader.destroy(id)));
         }
     }
  } catch (err) {
      console.error("Error processing journal deletions:", err);
  }
}

// Helper for Singletons (Budget, Gamification, Timetable)
// These should only have ONE document per user.
// IMPORTANT: Uses delete+create to handle legacy ObjectId vs new string _id conflicts
async function applySingletonChanges(Model, userId, items) {
  if (!items || items.length === 0) return;

  // We only care about the latest one if multiple are sent
  const latest = items.sort((a, b) => {
    const tA = a.updatedAt ? new Date(a.updatedAt).getTime() : 0;
    const tB = b.updatedAt ? new Date(b.updatedAt).getTime() : 0;
    return tB - tA;
  })[0];
  
  // Extract data but REMOVE _id to behave as a proper update
  // We do not want to overwrite the MongoDB _id with a generic client ID
  const { _id, ...data } = latest;
  data.userId = userId;
  data.updatedAt = new Date();

  // STEP 1: Get existing document to merge schedule if needed
  let existing = await Model.findOne({ userId: userId }).lean();
  
  // STEP 2: For Timetable - merge existing schedule with new schedule
  if (data.schedule && existing && existing.schedule) {
    const mergedSchedule = { ...existing.schedule };
    for (const [dayKey, dayValue] of Object.entries(data.schedule)) {
      mergedSchedule[dayKey] = dayValue;
    }
    data.schedule = mergedSchedule;
  } else if (!data.schedule && existing && existing.schedule) {
    // If new data doesn't have schedule, preserve existing
    data.schedule = existing.schedule;
  }

  // STEP 3: Upsert the document (Create if new, Update if exists)
  // This avoids duplicate key errors because we rely on unique ObjectId generated by Mongo
  try {
    await Model.findOneAndUpdate(
      { userId: userId },
      { $set: data },
      { upsert: true, new: true, setDefaultsOnInsert: true }
    );
  } catch (err) {
    console.error(`Failed to update/create ${Model.modelName}:`, err.message);
  }
}

// Helper to process "Push" updates efficiently using MongoDB bulkWrite
async function applyChanges(Model, userId, items) {
  if (!items || items.length === 0) return;

  const operations = items.map(item => {
    const { _id, ...data } = item;
    
    // Security: Ensure we don't overwrite the userId with something insecure
    data.userId = userId;
    // Force the server to set the update time, so all devices agree on timeline
    data.updatedAt = new Date();

    // Special handling for Subject history: merge instead of replace
    // This ensures attendance data from multiple devices is combined, not overwritten
    if (data.history && typeof data.history === 'object' && Object.keys(data.history).length > 0) {
      // Use $set with dot notation to merge history keys
      const historyUpdates = {};
      for (const [dateKey, dateValue] of Object.entries(data.history)) {
        historyUpdates[`history.${dateKey}`] = dateValue;
      }
      delete data.history; // Remove from $set since we're using dot notation
      
      return {
        updateOne: {
          filter: { _id: _id, userId: userId },
          update: { 
            $set: { ...data, ...historyUpdates }
          },
          upsert: true
        }
      };
    }

    return {
      updateOne: {
        filter: { _id: _id, userId: userId }, // Security: Only update if belongs to user
        update: { $set: data },
        upsert: true // If it doesn't exist (new item), create it
      }
    };
  });

  await Model.bulkWrite(operations);
}

// MAIN SYNC ROUTE
// POST /api/sync
router.post('/', authMiddleware, async (req, res) => {
  try {
    const userId = req.user._id;
    
    // 1. Get the last time this device synced
    const { lastSync, changes } = req.body; 
    
    const lastSyncDate = lastSync ? new Date(lastSync) : new Date(0);
    const currentSyncTime = new Date();

    // 2. PUSH: Apply changes from the client to MongoDB
    if (changes) {
      await Promise.all([
        applyChanges(Task, userId, changes.tasks),
        applyChanges(Habit, userId, changes.habits),
        applyChanges(Transaction, userId, changes.transactions),
        // Process Journal Deletions FIRST to clean up images
        processJournalDeletions(userId, changes.journal).then(() => 
             applyChanges(Journal, userId, changes.journal)
        ),
        // New Collections
        // New Collections
        applyChanges(Subject, userId, changes.subjects),
        applySingletonChanges(Timetable, userId, changes.timetable),
        applyChanges(BucketItem, userId, changes.bucketList),
        applySingletonChanges(Gamification, userId, changes.gamification),
        applySingletonChanges(Budget, userId, changes.budget) // <-- NEW 
      ]);
    }

    // 3. PULL: Fetch new data from MongoDB that this device doesn't have yet
    // logic: "Give me everything where updatedAt > my lastSyncDate"
    const [
      tasks, 
      habits, 
      transactions, 
      journal, 
      subjects, 
      timetable, 
      bucketList,
      gamification,
      budget
    ] = await Promise.all([
      Task.find({ userId, updatedAt: { $gt: lastSyncDate } }).lean(),
      Habit.find({ userId, updatedAt: { $gt: lastSyncDate } }).lean(),
      Transaction.find({ userId, updatedAt: { $gt: lastSyncDate } }).lean(),
      Journal.find({ userId, updatedAt: { $gt: lastSyncDate } }).lean(),
      // New Collections
      Subject.find({ userId, updatedAt: { $gt: lastSyncDate } }).lean(),
      Timetable.find({ userId, updatedAt: { $gt: lastSyncDate } }).lean(),
      BucketItem.find({ userId, updatedAt: { $gt: lastSyncDate } }).lean(),
      Gamification.find({ userId, updatedAt: { $gt: lastSyncDate } }).lean(),
      Budget.find({ userId, updatedAt: { $gt: lastSyncDate } }).lean() // <-- NEW 
    ]);

    // 4. Update User's lastSync field (for analytics/debugging only)
    req.user.lastSync = currentSyncTime;
    await req.user.save();

    // 5. Respond
    res.json({
      success: true,
      timestamp: currentSyncTime, // Client must save this for the next sync!
      changes: {
        tasks,
        habits,
        transactions,
        journal,
        subjects,    
        timetable,   
        bucketList,   
        gamification,
        budget  
      }
    });

  } catch (error) {
    console.error('Sync Error:', error);
    res.status(500).json({ error: 'Sync failed' });
  }
});

// RESET ROUTE
// DELETE /api/sync/reset
router.delete('/reset', authMiddleware, async (req, res) => {
  try {
    const userId = req.user._id;
    console.log(`⚠️ RESETTING DATA FOR USER: ${userId}`);

    await Promise.all([
      Task.deleteMany({ userId }),
      Habit.deleteMany({ userId }),
      Transaction.deleteMany({ userId }),
      Journal.deleteMany({ userId }),
      Subject.deleteMany({ userId }),
      Timetable.deleteMany({ userId }),
      BucketItem.deleteMany({ userId }),
      Gamification.deleteMany({ userId }),
      Budget.deleteMany({ userId }),
      // Socials
      SocialPost.deleteMany({ authorId: userId }),
      SocialGroup.deleteMany({ ownerId: userId }), // Only groups owned by user
      // SplitFund
      SplitGroup.deleteMany({ ownerId: userId }),
      SplitExpense.deleteMany({ paidBy: userId }), // Technically expenses could be tricky if group persists, but for "wipe everything" this is safer
      // Ideally for SplitFund, if you wipe your data, you might want to leave groups you don't own? 
      // But "Clear Database" usually implies wiping ALL your footprint.
    ]);

    // Reset Last Sync
    req.user.lastSync = new Date(0);
    await req.user.save();

    res.json({ success: true, message: 'All data cleared from server.' });
  } catch (error) {
    console.error('Reset Error:', error);
    res.status(500).json({ error: 'Reset failed' });
  }
});

module.exports = router;